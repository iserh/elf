@article{TACL1244,
	author = {Ryan Gallagher and Kyle Reing and David Kale and Greg Ver Steeg},
	title = {Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {5},
	number = {0},
	year = {2017},
	keywords = {},
	abstract = {While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize generative models to incorporate human input. We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an information-theoretic framework. This framework naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.},
	issn = {2307-387X},	pages = {529--542},	url = {https://transacl.org/ojs/index.php/tacl/article/view/1244}
}

@article{DBLP:journals/corr/abs-1908-10084,
  author    = {Nils Reimers and
               Iryna Gurevych},
  title     = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  journal   = {CoRR},
  volume    = {abs/1908.10084},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10084},
  eprinttype = {arXiv},
  eprint    = {1908.10084},
  timestamp = {Thu, 26 Nov 2020 12:13:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10084.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.5555/944919.944937, author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.}, title = {Latent Dirichlet Allocation}, year = {2003}, issue_date = {3/1/2003}, publisher = {JMLR.org}, volume = {3}, number = {null}, issn = {1532-4435}, abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.}, journal = {J. Mach. Learn. Res.}, month = {mar}, pages = {993â€“1022}, numpages = {30} }
