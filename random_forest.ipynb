{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d936036",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae80a0-5b64-4fdc-a194-01b12a52297f",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bba398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iai/user/iser/.conda/envs/main/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from utils import corex_probs_factory, preprocess_factory, syntax_factory\n",
    "\n",
    "data_dir = Path(\"/home/iailab36/iser/data\")\n",
    "model_dir = Path(\"/home/iailab36/iser/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3639de72-d37d-43a6-a3b9-b0dd80c8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark dataset\n",
    "train_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"sts-train.feather\")\n",
    "val_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"sts-dev.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932b988-63fc-4b0d-9da6-7b1f28009044",
   "metadata": {},
   "source": [
    "## Pre-compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd268a3-0bc4-4346-82c3-af4fc9786a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = []\n",
    "features_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d9631-bda1-404d-82b6-de3cea1ea494",
   "metadata": {},
   "source": [
    "### topic model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593b88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_probs = corex_probs_factory(\n",
    "    corex_path=model_dir / \"corex-sts-128\",\n",
    "    vectorizer_path=model_dir / \"vectorizer-sts-10000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8023d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute topic probabilities\n",
    "topic_probs_train_1 = get_topic_probs(train_data.s1)\n",
    "topic_probs_train_2 = get_topic_probs(train_data.s2)\n",
    "topic_probs_val_1 = get_topic_probs(val_data.s1)\n",
    "topic_probs_val_2 = get_topic_probs(val_data.s2)\n",
    "# concatenate topics of the two sentences\n",
    "topic_probs_train = np.concatenate([topic_probs_train_1, topic_probs_train_2], axis=1)\n",
    "topic_probs_val = np.concatenate([topic_probs_val_1, topic_probs_val_2], axis=1)\n",
    "# add to features list\n",
    "features_train.append(topic_probs_train)\n",
    "features_val.append(topic_probs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5e1bd-6f65-416a-ae66-d30480beb0ac",
   "metadata": {},
   "source": [
    "### syntax features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_syntax_deps = syntax_factory()\n",
    "# compute syntax tokens\n",
    "syntax_train_1 = get_syntax_deps(train_data.s1)\n",
    "syntax_train_2 = get_syntax_deps(train_data.s2)\n",
    "syntax_val_1 = get_syntax_deps(val_data.s1)\n",
    "syntax_val_2 = get_syntax_deps(val_data.s2)\n",
    "# mask matching syntax\n",
    "syntax_train = (syntax_train_1 == syntax_train_2).astype(int)\n",
    "syntax_val = (syntax_val_1 == syntax_val_2).astype(int)\n",
    "# append to features list\n",
    "features_train.append(syntax_train)\n",
    "features_val.append(syntax_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb0237",
   "metadata": {},
   "source": [
    "## Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c54bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5552, 256)\n"
     ]
    }
   ],
   "source": [
    "# create input vectors\n",
    "X_train = np.concatenate(features_train, axis=1)\n",
    "X_val = np.concatenate(features_val, axis=1)\n",
    "# create targets\n",
    "y_train = train_data.score\n",
    "y_val = val_data.score\n",
    "print(\"X_train:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539bac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanRank-train: 0.9681,\t SpearmanRank-val: 0.5079\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = RandomForestRegressor(criterion=\"squared_error\", n_estimators=100, max_depth=15, random_state=1337)\n",
    "# model = DecisionTreeRegressor(random_state=1337)\n",
    "# model = MLPRegressor((1024, 512, 256, 128))\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate model\n",
    "spearman_train = spearmanr(model.predict(X_train), y_train)[0]\n",
    "spearman_val = spearmanr(model.predict(X_val), y_val)[0]\n",
    "print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-val: {spearman_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f60264",
   "metadata": {},
   "source": [
    "Only STS-Benchmark:\n",
    "```python\n",
    "spearmanrank_wo_syntax = 0.4593\n",
    "spearmanrank_w_syntax = 0.4541\n",
    "```\n",
    "\n",
    "STS-B + NLI:\n",
    "```python\n",
    "spearmanrank_wo_syntax = 0.4586\n",
    "spearmanrank_w_syntax = 0.4614\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b30fd-3063-4941-9b5c-8ba14e948361",
   "metadata": {},
   "source": [
    "## Training with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4605f626-fdda-4253-81aa-e58530d6c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load augmentation dataset\n",
    "# aug_data = pd.read_feather(benchmark_dir / \"df_augment.feather\")\n",
    "\n",
    "# features_aug = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc96d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get topics of the augmented sentences\n",
    "# topic_probs_augmented = np.concatenate([\n",
    "#     topic_probs_train_1[aug_data.idx1],\n",
    "#     topic_probs_train_2[aug_data.idx2]\n",
    "# ], axis=1)\n",
    "# features_aug.append(topic_probs_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aef96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax_aug = (syntax_train_1[aug_data.idx1] == syntax_train_2[aug_data.idx2]).astype(int)\n",
    "# features_aug.append(syntax_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f11a8e5-2a08-4886-9839-72c29bb8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create inputs / targets of augmented dataset\n",
    "# X_aug = np.concatenate(features_aug, axis=1)\n",
    "# y_aug = aug_data.score\n",
    "# print(f\"#augmented: {y_aug.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f969821a-3475-4763-9b32-9e04b85f497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_w_aug = np.concatenate([X_train, X_aug])\n",
    "# y_train_w_aug = np.concatenate([y_train, y_aug])\n",
    "# print(f\"#(train+augmented): {y_aug.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "090560af-8f6b-4a62-9439-639897783d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model\n",
    "# model = RandomForestRegressor(criterion=\"squared_error\", n_estimators=100, max_depth=15, random_state=1337)\n",
    "# model.fit(X_train_w_aug, y_train_w_aug)\n",
    "# # evaluate model\n",
    "# spearman_train = spearmanr(model.predict(X_train_w_aug), y_train_w_aug)[0]\n",
    "# spearman_test = spearmanr(model.predict(X_val), y_val)[0]\n",
    "# print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-val: {spearman_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320adabc-693c-4e1a-97e6-fe99fb6e27b5",
   "metadata": {},
   "source": [
    "```python\n",
    "SpearmanRank = 0.4677\n",
    "np.random.seed(??)\n",
    "f = lambda x: np.exp(3*x) * 2  # sampling function - #1228\n",
    "\n",
    "SpearmanRank = 0.4557\n",
    "np.random.seed(42)\n",
    "f = lambda x: np.exp(3*x) * 2  # sampling function - #1228\n",
    "\n",
    "SpearmanRank = 0.4374\n",
    "np.random.seed(1337)\n",
    "f = lambda x: np.exp(4*x) * 2  # sampling function - #2655\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683639f",
   "metadata": {},
   "source": [
    "## Qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df8f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628fd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = preprocess_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db264611-0f48-4b8b-978c-9c1b637472c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syntax_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1178674/2719103923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nsubj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dobj\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntax_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'syntax_val' is not defined"
     ]
    }
   ],
   "source": [
    "df = val_data[[\"s1\", \"s2\"]].copy()\n",
    "df[\"s1_processed\"] = df.s1.apply(preprocess)\n",
    "df[\"s2_processed\"] = df.s2.apply(preprocess)\n",
    "df[\"y_true\"] = y_val\n",
    "df[\"y_pred\"] = y_pred\n",
    "df[[\"root\", \"nsubj\", \"dobj\"]] = [pd.Series(s) for s in syntax_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b681c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.y_true - df.y_pred) > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b52d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_syntax_deps(val_data.s1)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71dd82-4907-44ff-a30e-6ec8ab295f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.y_true - df.y_pred) < 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e543c6b",
   "metadata": {},
   "source": [
    "### Naive cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tok2vec\"] = df[[\"s1\", \"s2\"]].apply(lambda row: nlp(row.s1).similarity(nlp(row.s2)), axis=1)\n",
    "print(f\"SpearmanRank-val: {spearmanr(df.tok2vec, df.y_true)[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Blue and red plane in mid-air flight.\")\n",
    "print(\"\\t\".join([token.dep_ for token in doc]))\n",
    "print(\"\\t\".join([token.lemma_ for token in doc]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2031cad5f309ecce3c7725635a5b7ce703908d09c65dbfde6df395d8b1d2cb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
