{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d936036",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae80a0-5b64-4fdc-a194-01b12a52297f",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5bba398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchmetrics import Accuracy, SpearmanCorrcoef\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sentence_similarity.data import (CoreXFeatures, LDAFeatures, PreprocessingModule,\n",
    "                                      STSBenchmark, SyntaxFeatures)\n",
    "from sentence_similarity.data import PipelineConfig, Pipeline\n",
    "from sentence_similarity.corex import train_corex_model\n",
    "\n",
    "data_dir = Path('data')\n",
    "model_dir = Path(\"models\")\n",
    "assert data_dir.exists()\n",
    "assert model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3639de72-d37d-43a6-a3b9-b0dd80c8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark dataset\n",
    "train_data = STSBenchmark(data_dir, partition=\"train\")\n",
    "val_data = STSBenchmark(data_dir, partition=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0177c",
   "metadata": {},
   "source": [
    "## Training the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "349e9aeb-829c-4c29-8bfc-8516067697b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(corex_feat.vectorizer.get_feature_names_out()))\n",
    "\"guitar.\" in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848cf10b-c109-44bf-8870-c60504453335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the nli dataset (train)\n",
    "# df_nli = pd.read_csv(data_dir / \"snli_1.0\" / \"snli_1.0_train.txt\", delimiter=\"\\t\")\n",
    "# sentences = pd.concat([df_nli.sentence1, train_data.s1, train_data.s2])\n",
    "sentences = pd.concat([train_data.s1, train_data.s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e1acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  40%|████████████▍                  | 4442/11104 [00:03<00:04, 1424.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21630/1417300209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# train corex topic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_corex_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"corex_nli_stsb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/explainable-linguistic-features/sentence_similarity/corex.py\u001b[0m in \u001b[0;36mtrain_corex_model\u001b[0;34m(pipeline, sentences, output_dir, n_hidden, max_iter, seed)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# preprocess sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# fit TF-IDF vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/explainable-linguistic-features/sentence_similarity/data/preprocess.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sentences, split_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# spacy processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Preprocessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             processed.append(\"\".join([\n\u001b[1;32m     67\u001b[0m                 \u001b[0;34mf\"{token.lemma_ if self.use_lemmas else token.text} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1576\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m                 \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/pipeline/pipe.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m ) -> Iterator[\"Doc\"]:\n\u001b[1;32m   1598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0;31m# We added some args for pipe that __call__ doesn't expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/pipeline/pipe.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/main/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_spans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Sort by the attribute ID, so that later rules have precedence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         matches = [\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize preprocessing pipeline\n",
    "config = PipelineConfig(\n",
    "    filtered_pos_tags=[],\n",
    "    use_lemmas=True,\n",
    "    remove_stop_words=True,\n",
    "    remove_numbers=False,\n",
    "    remove_symbols=False,\n",
    "    remove_punctuation=False,\n",
    ")\n",
    "pipeline = Pipeline(config)\n",
    "# train corex topic model\n",
    "train_corex_model(pipeline, sentences, model_dir / \"corex_nli_stsb\", n_hidden=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932b988-63fc-4b0d-9da6-7b1f28009044",
   "metadata": {},
   "source": [
    "## Pre-compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd268a3-0bc4-4346-82c3-af4fc9786a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = []\n",
    "features_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d9631-bda1-404d-82b6-de3cea1ea494",
   "metadata": {},
   "source": [
    "### topic model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "593b88dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#topics: 50\n"
     ]
    }
   ],
   "source": [
    "# features for the model\n",
    "corex_feat = CoreXFeatures(model_dir / \"corex_nli_stsb\")\n",
    "print(f\"#topics: {corex_feat.input_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute topic probabilities\n",
    "topic_probs_train_1 = corex_feat(train_data.s1)\n",
    "topic_probs_train_2 = corex_feat(train_data.s2)\n",
    "topic_probs_val_1 = corex_feat(val_data.s1)\n",
    "topic_probs_val_2 = corex_feat(val_data.s2)\n",
    "# concatenate topics of the two sentences\n",
    "topic_probs_train = np.concatenate([topic_probs_train_1, topic_probs_train_2], axis=1)\n",
    "topic_probs_val = np.concatenate([topic_probs_val_1, topic_probs_val_2], axis=1)\n",
    "# add to features list\n",
    "features_train.append(topic_probs_train)\n",
    "features_val.append(topic_probs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5e1bd-6f65-416a-ae66-d30480beb0ac",
   "metadata": {},
   "source": [
    "### syntax features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax_feat = SyntaxFeatures()\n",
    "# # compute syntax tokens\n",
    "# syntax_train_1 = syntax_feat(train_data.s1)\n",
    "# syntax_train_2 = syntax_feat(train_data.s2)\n",
    "# syntax_val_1 = syntax_feat(val_data.s1)\n",
    "# syntax_val_2 = syntax_feat(val_data.s2)\n",
    "# # mask matching syntax\n",
    "# syntax_train = (syntax_train_1 == syntax_train_2).astype(int)\n",
    "# syntax_val = (syntax_val_1 == syntax_val_2).astype(int)\n",
    "# # append to features list\n",
    "# features_train.append(syntax_train)\n",
    "# features_val.append(syntax_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb0237",
   "metadata": {},
   "source": [
    "## Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c54bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input vectors\n",
    "X_train = np.concatenate(features_train, axis=1)\n",
    "X_val = np.concatenate(features_val, axis=1)\n",
    "# create targets\n",
    "y_train = train_data.score\n",
    "y_val = val_data.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = RandomForestRegressor(criterion=\"squared_error\", n_estimators=100, max_depth=15, random_state=1337)\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate model\n",
    "spearman_train = spearmanr(model.predict(X_train), y_train)[0]\n",
    "spearman_val = spearmanr(model.predict(X_val), y_val)[0]\n",
    "print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-val: {spearman_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f60264",
   "metadata": {},
   "source": [
    "Only STS-Benchmark:\n",
    "```python\n",
    "spearmanrank_wo_syntax = 0.4593\n",
    "spearmanrank_w_syntax = 0.4541\n",
    "```\n",
    "\n",
    "STS-B + NLI:\n",
    "```python\n",
    "spearmanrank_wo_syntax = 0.4586\n",
    "spearmanrank_w_syntax = 0.4614\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b30fd-3063-4941-9b5c-8ba14e948361",
   "metadata": {},
   "source": [
    "## Training with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605f626-fdda-4253-81aa-e58530d6c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmentation dataset\n",
    "augmentation_data = pd.read_feather(data_dir / \"df_augment.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11a8e5-2a08-4886-9839-72c29bb8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics of the augmented sentences\n",
    "topic_probs_augmented = np.concatenate([\n",
    "    topic_probs_train_1[augmentation_data.idx1],\n",
    "    topic_probs_train_2[augmentation_data.idx2]\n",
    "], axis=1)\n",
    "\n",
    "# get dependencies of the augmented sentences\n",
    "syntax_augmented = syntax_train_1[augmentation_data.idx1] == syntax_train_2[augmentation_data.idx2]\n",
    "\n",
    "# create inputs / targets of augmented dataset\n",
    "X_augmented = topic_probs_augmented  # np.concatenate([topic_probs_augmented, syntax_augmented], axis=1)\n",
    "y_augmented = augmentation_data.score\n",
    "print(f\"#augmented: {y_augmented.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969821a-3475-4763-9b32-9e04b85f497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w_augment = np.concatenate([X_train, X_augmented])\n",
    "y_train_w_augment = np.concatenate([y_train, y_augmented])\n",
    "print(f\"#(train+augmented): {y_augmented.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090560af-8f6b-4a62-9439-639897783d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = RandomForestRegressor(criterion=\"squared_error\", n_estimators=100, max_depth=15, random_state=1337)\n",
    "model.fit(X_train_w_augment, y_train_w_augment)\n",
    "# evaluate model\n",
    "spearman_train = spearmanr(model.predict(X_train_w_augment), y_train_w_augment)[0]\n",
    "spearman_test = spearmanr(model.predict(X_val), y_val)[0]\n",
    "print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-val: {spearman_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320adabc-693c-4e1a-97e6-fe99fb6e27b5",
   "metadata": {},
   "source": [
    "```python\n",
    "SpearmanRank = 0.4677\n",
    "np.random.seed(??)\n",
    "f = lambda x: np.exp(3*x) * 2  # sampling function - #1228\n",
    "\n",
    "SpearmanRank = 0.4557\n",
    "np.random.seed(42)\n",
    "f = lambda x: np.exp(3*x) * 2  # sampling function - #1228\n",
    "\n",
    "SpearmanRank = 0.4374\n",
    "np.random.seed(1337)\n",
    "f = lambda x: np.exp(4*x) * 2  # sampling function - #2655\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683639f",
   "metadata": {},
   "source": [
    "## Qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db264611-0f48-4b8b-978c-9c1b637472c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = val_data.df[[\"s1\", \"s2\"]].copy()\n",
    "df[\"s1_processed\"] = pipeline(df.s1)\n",
    "df[\"s2_processed\"] = pipeline(df.s2)\n",
    "df[\"y_true\"] = y_val\n",
    "df[\"y_pred\"] = y_pred\n",
    "df[[\"root\", \"nsubj\", \"dobj\"]] = [pd.Series(s) for s in syntax_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71dd82-4907-44ff-a30e-6ec8ab295f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.y_pred > df.y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc244-c6ec-4016-abc1-9825d49fdb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df.y_pred.argmax()].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b681c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.y_true - df.y_pred) > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc82a9-5266-4ac6-aab8-a7cb4f99ff56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e543c6b",
   "metadata": {},
   "source": [
    "### Naive cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tok2vec\"] = df[[\"s1\", \"s2\"]].apply(lambda row: nlp(row.s1).similarity(nlp(row.s2)), axis=1)\n",
    "print(f\"SpearmanRank-val: {spearmanr(df.tok2vec, df.y_true)[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Blue and red plane in mid-air flight.\")\n",
    "print(\"\\t\".join([token.dep_ for token in doc]))\n",
    "print(\"\\t\".join([token.lemma_ for token in doc]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2031cad5f309ecce3c7725635a5b7ce703908d09c65dbfde6df395d8b1d2cb2"
  },
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
