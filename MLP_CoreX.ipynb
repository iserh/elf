{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d936036",
   "metadata": {},
   "source": [
    "# Training (small) model build on CoreX topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0a5c7",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bba398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torchmetrics import Accuracy, SpearmanCorrcoef\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sentence_similarity.data import (CoreXFeatures, LDAFeatures, PreprocessingModule,\n",
    "                                      STSBenchmark, SyntaxFeatures)\n",
    "\n",
    "data_dir = Path('data')\n",
    "assert data_dir.exists()\n",
    "output_dir = Path('data/output')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0177c",
   "metadata": {},
   "source": [
    "## Data Preprocessing with CoreX topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e1acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 5552/5552 [00:03<00:00, 1486.40it/s]\n",
      "Preprocessing: 100%|██████████| 5552/5552 [00:03<00:00, 1823.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# train corex topic model\n",
    "from sentence_similarity.data import PipelineConfig\n",
    "from sentence_similarity.corex import train_corex_model\n",
    "\n",
    "benchmark = STSBenchmark(data_dir, partition=\"train\")\n",
    "config = PipelineConfig(\n",
    "    filtered_pos_tags=[],\n",
    "    use_lemmas=True,\n",
    "    remove_stop_words=True,\n",
    "    remove_numbers=False,\n",
    "    remove_symbols=False,\n",
    "    remove_punctuation=False,\n",
    ")\n",
    "train_corex_model(data_dir, config, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PreprocessingModule(data_dir, benchmark=STSBenchmark)\n",
    "data_module.prepare_data()\n",
    "corex_feat = CoreXFeatures(data_dir)\n",
    "print(\"#CoreX features:\", corex_feat.input_size)\n",
    "syntax_feat = SyntaxFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module.setup(feature_generators=[corex_feat, syntax_feat], target_transform=lambda y: torch.LongTensor(y > 0.5))\n",
    "data_module.setup(feature_generators=[corex_feat, syntax_feat], target_transform=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768904f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_augmentation = pd.read_feather(data_dir / \"df_augment.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb0237",
   "metadata": {},
   "source": [
    "## Training without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, output_size: int = 1, final_act: nn.Module = nn.Tanh()):\n",
    "        super().__init__()\n",
    "        # embedding parameter\n",
    "        self.topic = nn.Sequential(\n",
    "            nn.Linear(num_embeddings * 2, embedding_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # syntax input processing\n",
    "        self.syntax_dep = nn.Sequential(\n",
    "            nn.Linear(3, embedding_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # main net\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(num_embeddings * 2, embedding_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_size),\n",
    "            # nn.Tanh(),\n",
    "        )\n",
    "        # initialize\n",
    "        self.topic.apply(init_weights)\n",
    "        self.seq.apply(init_weights)\n",
    "\n",
    "    def forward(self, a, b) -> torch.Tensor:\n",
    "        # extract features\n",
    "        topic = torch.cat([a[0], b[0]], dim=1)\n",
    "        # root = (a[1][:, 0] == b[1][:, 0]).float().unsqueeze(1)\n",
    "        # nsubj = (a[1][:, 1] == b[1][:, 1]).float().unsqueeze(1)\n",
    "        # dobj = (a[1][:, 2] == b[1][:, 2]).float().unsqueeze(1)\n",
    "        # # topic input layer - shape: [batch, embedding_dim]\n",
    "        # topic = self.topic(topic)\n",
    "        # # syntax input layer - shape: [batch, embedding_dim]\n",
    "        # syntax_dep = self.syntax_dep(torch.cat([root, nsubj, dobj], dim=1))\n",
    "        # # input for sequential net - shape: [batch, embedding_dim * 2]\n",
    "        # x = torch.cat([topic, syntax_dep], dim=1)\n",
    "        # run sequential net - output shape: [batch,]\n",
    "        return self.seq(topic).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimilarityModel(num_embeddings=data_module.feature_generators[0].input_size, embedding_dim=512)\n",
    "batch = next(iter(data_module.train_dataloader()))\n",
    "model(*batch[:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdffc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 4_000\n",
    "V_INTERVAL = 20\n",
    "\n",
    "model = SimilarityModel(num_embeddings=data_module.feature_generators[0].input_size, embedding_dim=1024)  #, output_size=2, final_act=nn.Identity())\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)  # , weight_decay=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "metric = SpearmanCorrcoef()\n",
    "\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "val_dataloader = data_module.test_dataloader()\n",
    "\n",
    "train_losses, train_spearman = [], []\n",
    "test_losses, test_spearman = [], []\n",
    "running_loss = 0\n",
    "step = 0\n",
    "\n",
    "# training\n",
    "with tqdm(total=N_STEPS) as pbar:\n",
    "    while step < N_STEPS:\n",
    "        for feat1, feat2, y_hat in train_dataloader:\n",
    "            # increase step\n",
    "            step += 1\n",
    "            if step > N_STEPS:\n",
    "                break\n",
    "            # compute embeddings for both input sentences\n",
    "            y = model(feat1, feat2)\n",
    "            # compute loss\n",
    "            loss = criterion(y, y_hat)\n",
    "            # loss, metric tracking\n",
    "            running_loss += loss.item()\n",
    "            metric(y, y_hat)\n",
    "            # optimizer step\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # metric logging\n",
    "            if step % V_INTERVAL == 0:\n",
    "                # Test epoch\n",
    "                with torch.no_grad():\n",
    "                    y, y_hat = [*zip(*[(model(feat1, feat2), score) for feat1, feat2, score in val_dataloader])]\n",
    "                y = torch.cat(y)\n",
    "                y_hat = torch.cat(y_hat)\n",
    "                # append metrics\n",
    "                train_losses.append(running_loss / V_INTERVAL)\n",
    "                train_spearman.append(metric.compute())\n",
    "                test_losses.append(criterion(y, y_hat))\n",
    "                test_spearman.append(metric(y, y_hat))\n",
    "                # reset tracking\n",
    "                running_loss = 0\n",
    "                metric.reset()\n",
    "                # update progress bar\n",
    "                pbar.set_postfix({\"loss\": train_losses[-1], \"train\": train_spearman[-1], \"test\": test_spearman[-1]})\n",
    "\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f60264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(train_losses, color=\"tab:blue\", alpha=0.2)\n",
    "ax[0].plot(pd.Series(train_losses).rolling(window=N).mean().iloc[N-1:].values, color=\"tab:blue\")\n",
    "ax[0].set_title(\"loss vs. steps\")\n",
    "ax[1].plot(train_spearman, color=\"tab:blue\", alpha=0.2)\n",
    "ax[1].plot(pd.Series(train_spearman).rolling(window=N).mean().iloc[N-1:].values, color=\"tab:blue\", label=f\"train ({max(train_spearman):.3f})\")\n",
    "ax[1].plot(test_spearman, color=\"tab:orange\", alpha=0.2)\n",
    "ax[1].plot(pd.Series(test_spearman).rolling(window=N).mean().iloc[N-1:].values, color=\"tab:orange\", label=f\"test ({max(test_spearman):.3f})\")\n",
    "ax[1].set_title(\"spearman-rank vs. steps\")\n",
    "ax[1].legend()\n",
    "plt.savefig(output_dir / \"corex-linear-w-augmentation-3.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683639f",
   "metadata": {},
   "source": [
    "## Analyse Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score test dataset\n",
    "with torch.no_grad():\n",
    "    y, y_hat = [*zip(*[(model(featA, featB), score) for featA, featB, score in val_dataloader])]\n",
    "y = torch.cat(y)\n",
    "y_hat = torch.cat(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d73917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_module.intermediates[\"test\"].copy().drop(columns=\"score\")\n",
    "df[[\"root\", \"nsubj\", \"dobj\"]] = df[[\"feat1\", \"feat2\"]].apply(lambda row: pd.Series((row.feat1[1] == row.feat2[1]).long()), axis=1)\n",
    "df = df.drop(columns=[\"feat1\", \"feat2\"])\n",
    "df[\"y\"] = y\n",
    "df[\"y_hat\"] = y_hat\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b681c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.y_hat - df.y) > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "((df.root & df.nsubj).astype(int) == df.y_hat).sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"y_\"] = (df.root | df.nsubj).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e05b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(np.asarray(corex_feat.vectorizer.get_feature_names_out()))\n",
    "corex_feat.corex.set_words(words)\n",
    "topics = [[*zip(*topic)][0] for topic in corex_feat.corex.get_topics()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, feat2 = [*zip(*[(feat1[0], feat2[0]) for feat1, feat2, _ in val_dataloader])]\n",
    "feat1 = torch.cat(feat1).softmax(-1)\n",
    "feat2 = torch.cat(feat2).softmax(-1)\n",
    "feat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics:\n",
    "    if \"football\" in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.y_hat - df.y).abs() > 0.5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e543c6b",
   "metadata": {},
   "source": [
    "### Naive cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"naive\"] = df[[\"s1\", \"s2\"]].apply(lambda row: nlp(row.s1).similarity(nlp(row.s2)), axis=1)\n",
    "df[(df.y_hat - df.y).abs() > 0.5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SpearmanCorrcoef()(torch.FloatTensor(df.naive), torch.FloatTensor(df.y_hat)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"A woman measures another woman's ankle.\")\n",
    "print([token.dep_ for token in doc])\n",
    "print([token.lemma_ for token in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
