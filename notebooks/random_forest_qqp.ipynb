{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d936036",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae80a0-5b64-4fdc-a194-01b12a52297f",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bba398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from utils import (CoreXProbsFactory, LDAProbs, SyntaxFactory, preprocess,\n",
    "                   tokenize)\n",
    "\n",
    "data_dir = Path(\"/home/iailab36/iser/data\")\n",
    "model_dir = Path(\"/home/iailab36/iser/models\")\n",
    "\n",
    "SEED = 1337\n",
    "COREX_HIDDEN = 64\n",
    "VEC_FEAT = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3639de72-d37d-43a6-a3b9-b0dd80c8e6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/iailab36/iser/data/stsbenchmark/sts-train-sbert.feather'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/iai/user/iser/dev/explainable-linguistic-features/random_forest_qqp.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biailab36/home/iai/user/iser/dev/explainable-linguistic-features/random_forest_qqp.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Load benchmark dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biailab36/home/iai/user/iser/dev/explainable-linguistic-features/random_forest_qqp.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_feather(data_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mstsbenchmark\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msts-train-sbert.feather\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biailab36/home/iai/user/iser/dev/explainable-linguistic-features/random_forest_qqp.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m val_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_feather(data_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstsbenchmark\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msts-dev-sbert.feather\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biailab36/home/iai/user/iser/dev/explainable-linguistic-features/random_forest_qqp.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_feather(data_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstsbenchmark\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msts-test.feather\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py:128\u001b[0m, in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=124'>125</a>\u001b[0m import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=125'>126</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mimport\u001b[39;00m feather\n\u001b[0;32m--> <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=127'>128</a>\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=128'>129</a>\u001b[0m     path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=129'>130</a>\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=131'>132</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m feather\u001b[39m.\u001b[39mread_feather(\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=132'>133</a>\u001b[0m         handles\u001b[39m.\u001b[39mhandle, columns\u001b[39m=\u001b[39mcolumns, use_threads\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(use_threads)\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/feather_format.py?line=133'>134</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=798'>799</a>\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    <a href='file:///home/iai/user/iser/.conda/envs/lab-cpu/lib/python3.8/site-packages/pandas/io/common.py?line=800'>801</a>\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/iailab36/iser/data/stsbenchmark/sts-train-sbert.feather'"
     ]
    }
   ],
   "source": [
    "# Load benchmark dataset\n",
    "train_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"sts-train-sbert.feather\")\n",
    "val_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"sts-dev-sbert.feather\")\n",
    "test_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"sts-test.feather\")\n",
    "\n",
    "train_data = pd.concat([train_data, val_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = lambda SEED: RandomForestRegressor(criterion=\"squared_error\", n_estimators=100, random_state=SEED)\n",
    "decision_tree = lambda SEED: DecisionTreeRegressor(random_state=SEED, max_depth=5)\n",
    "mlp = lambda SEED: MLPRegressor((512, 256), random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932b988-63fc-4b0d-9da6-7b1f28009044",
   "metadata": {},
   "source": [
    "## Pre-compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd268a3-0bc4-4346-82c3-af4fc9786a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = []\n",
    "features_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d9631-bda1-404d-82b6-de3cea1ea494",
   "metadata": {},
   "source": [
    "### topic model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_probs = CoreXProbsFactory(\n",
    "    vectorizer_path=model_dir / f\"sts_vec={VEC_FEAT}\",\n",
    "    corex_name=f\"corex_n_hidden={COREX_HIDDEN}_iter=7\",\n",
    ")\n",
    "\n",
    "# get_topic_probs = LDAProbs(model_dir / f\"sts_lda_hidden={COREX_HIDDEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute topic probabilities\n",
    "topic_probs_train_1 = get_topic_probs(train_data.s1)\n",
    "topic_probs_train_2 = get_topic_probs(train_data.s2)\n",
    "topic_probs_test_1 = get_topic_probs(test_data.s1)\n",
    "topic_probs_test_2 = get_topic_probs(test_data.s2)\n",
    "# concatenate topics of the two sentences\n",
    "topic_probs_train = np.concatenate([topic_probs_train_1, topic_probs_train_2], axis=1)\n",
    "topic_probs_test = np.concatenate([topic_probs_test_1, topic_probs_test_2], axis=1)\n",
    "# add to features list\n",
    "features_train.append(topic_probs_train)\n",
    "features_val.append(topic_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5e1bd-6f65-416a-ae66-d30480beb0ac",
   "metadata": {},
   "source": [
    "### syntax features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_syntax_deps = SyntaxFactory()\n",
    "# # compute syntax tokens\n",
    "# syntax_train_1 = get_syntax_deps(train_data.s1)\n",
    "# syntax_train_2 = get_syntax_deps(train_data.s2)\n",
    "# syntax_test_1 = get_syntax_deps(test_data.s1)\n",
    "# syntax_test_2 = get_syntax_deps(test_data.s2)\n",
    "# # mask matching syntax\n",
    "# syntax_train = (syntax_train_1 == syntax_train_2).astype(int)\n",
    "# syntax_test = (syntax_test_1 == syntax_test_2).astype(int)\n",
    "# # append to features list\n",
    "# features_train.append(syntax_train)\n",
    "# features_val.append(syntax_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb0237",
   "metadata": {},
   "source": [
    "## Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c54bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input vectors\n",
    "X_train = np.concatenate(features_train, axis=1)\n",
    "X_test = np.concatenate(features_val, axis=1)\n",
    "# create targets\n",
    "y_train = train_data.score\n",
    "y_test = test_data.score\n",
    "print(\"X_train:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "perm = np.random.permutation(X_train.shape[0])\n",
    "X_train_ = X_train[perm]\n",
    "y_train_ = y_train[perm]\n",
    "\n",
    "model = random_forest(SEED)\n",
    "model.fit(X_train_, y_train_)\n",
    "\n",
    "# evaluate model\n",
    "spearman_train = spearmanr(model.predict(X_train), y_train)[0]\n",
    "spearman_test = spearmanr(model.predict(X_test), y_test)[0]\n",
    "\n",
    "print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-test: {spearman_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b30fd-3063-4941-9b5c-8ba14e948361",
   "metadata": {},
   "source": [
    "## Training with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605f626-fdda-4253-81aa-e58530d6c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmentation dataset\n",
    "aug_data = pd.read_feather(data_dir / \"stsbenchmark\" / \"df_augment.feather\")\n",
    "\n",
    "features_aug = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics of the augmented sentences\n",
    "topic_probs_augmented = np.concatenate([\n",
    "    topic_probs_train_1[aug_data.idx1],\n",
    "    topic_probs_train_2[aug_data.idx2]\n",
    "], axis=1)\n",
    "features_aug.append(topic_probs_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # syntax features\n",
    "# syntax_aug = (syntax_train_1[aug_data.idx1] == syntax_train_2[aug_data.idx2]).astype(int)\n",
    "# features_aug.append(syntax_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11a8e5-2a08-4886-9839-72c29bb8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs / targets of augmented dataset\n",
    "X_aug = np.concatenate(features_aug, axis=1)\n",
    "y_aug = aug_data.score\n",
    "print(f\"#augmented: {y_aug.shape[0]}\")\n",
    "\n",
    "X_train_w_aug = np.concatenate([X_train, X_aug])\n",
    "y_train_w_aug = np.concatenate([y_train, y_aug])\n",
    "print(f\"#(train+augmented): {y_train_w_aug.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(SEED)\n",
    "# perm = np.random.permutation(X_train_w_aug.shape[0])\n",
    "# X_train_w_aug_ = X_train_w_aug[perm]\n",
    "# y_train_w_aug_ = y_train_w_aug[perm]\n",
    "\n",
    "# model = random_forest(SEED)\n",
    "# model.fit(X_train_w_aug_, y_train_w_aug_)\n",
    "\n",
    "# # evaluate model\n",
    "# spearman_train = spearmanr(model.predict(X_train), y_train)[0]\n",
    "# spearman_test = spearmanr(model.predict(X_test), y_test)[0]\n",
    "\n",
    "# print(f\"SpearmanRank-train: {spearman_train:.4f},\\t SpearmanRank-test: {spearman_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683639f",
   "metadata": {},
   "source": [
    "## Qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca992f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unit vectors\n",
    "# x1 = topic_probs_train_1 / np.linalg.norm(topic_probs_train_1, axis=1)[:, None]\n",
    "# x2 = topic_probs_train_2 / np.linalg.norm(topic_probs_train_2, axis=1)[:, None]\n",
    "\n",
    "# y_naiv = (x1[:, None, ...] @ x2[..., None]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # softmax\n",
    "# x1 = np.exp(topic_probs_train_1)/np.exp(topic_probs_train_1).sum(0)\n",
    "# x2 = np.exp(topic_probs_train_2)/np.exp(topic_probs_train_2).sum(0)\n",
    "\n",
    "# dists = np.array([wasserstein_distance(x1[i], x2[i]) for i in range(x1.shape[0])])\n",
    "\n",
    "# mms = MinMaxScaler()\n",
    "# y_naiv = mms.fit_transform(dists.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd5f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404287\n",
      "323429\n",
      "80858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_qqp = pd.read_feather(data_dir / \"qqp.feather\")\n",
    "indices = np.arange(len(df_qqp))\n",
    "train_indices, test_indices = train_test_split(indices, stratify=df_qqp.score, test_size=0.2, train_size=0.8)\n",
    "\n",
    "train_data = df_qqp.iloc[train_indices]\n",
    "test_data = df_qqp.iloc[test_indices]\n",
    "\n",
    "y_test = test_data.score\n",
    "\n",
    "print(len(df_qqp))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d93703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What are different fields in computer science and which is better?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qqp[df_qqp.s2 == \"What are different fields for computer science?\"].s1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "958fe9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac97f391e8354bccbb8151b39ed4a593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/646858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf52448c243a4a6d98c392b9e75f94a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198e68d5c2164c81852df1d053d41d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sw = True\n",
    "vectorizer = CountVectorizer()\n",
    "tqdm.pandas()\n",
    "vectorizer.fit(pd.concat([train_data.s1, train_data.s2]).progress_apply(preprocess))\n",
    "bow1 = vectorizer.transform(test_data.s1.progress_apply(preprocess))\n",
    "bow2 = vectorizer.transform(test_data.s2.progress_apply(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c78a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import scipy.sparse as ss\n",
    "\n",
    "bow1: ss.csr_matrix = normalize(bow1, axis=1).toarray()\n",
    "bow2: ss.csr_matrix = normalize(bow2, axis=1).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b2e85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = (bow1[:, None, ...] @ bow2[..., None]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "747146a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6287194835390437"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_naiv = (score > 0.845).astype(int)\n",
    "\n",
    "(y_test == y_naiv).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db264611-0f48-4b8b-978c-9c1b637472c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data[[\"s1\", \"s2\"]].copy()\n",
    "df[\"s1_processed\"] = df.s1.apply(preprocess)\n",
    "df[\"s2_processed\"] = df.s2.apply(preprocess)\n",
    "df[\"y_true\"] = y_test\n",
    "# df[\"y_pred\"] = y_pred\n",
    "df[\"y_naiv\"] = y_naiv\n",
    "# df[[\"root\", \"nsubj\", \"dobj\"]] = [pd.Series(s) for s in syntax_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b681c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s1_processed</th>\n",
       "      <th>s2_processed</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_naiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166532</th>\n",
       "      <td>Why are people on this site so obsessed with IQ?</td>\n",
       "      <td>Why are most Quora users so obsessed with ques...</td>\n",
       "      <td>whi are peopl on this site so obsess with iq</td>\n",
       "      <td>whi are most quora user so obsess with questio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307615</th>\n",
       "      <td>Can your soul cause your body to explode?</td>\n",
       "      <td>Is it true that the soul can cause the body to...</td>\n",
       "      <td>can your soul caus your bodi to explod</td>\n",
       "      <td>is it true that the soul can caus the bodi to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171311</th>\n",
       "      <td>What are different fields in computer science ...</td>\n",
       "      <td>What are different fields for computer science?</td>\n",
       "      <td>what are differ field in comput scienc and whi...</td>\n",
       "      <td>what are differ field for comput scienc</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242855</th>\n",
       "      <td>How does it feel to have an IITian girlfriend?</td>\n",
       "      <td>How does it feel to have an IITian as a girlfr...</td>\n",
       "      <td>how doe it feel to have an iitian girlfriend</td>\n",
       "      <td>how doe it feel to have an iitian as a girlfriend</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187843</th>\n",
       "      <td>Have you ever had sex with your best friend?</td>\n",
       "      <td>Did you have sex with your best friend?</td>\n",
       "      <td>have you ever had sex with your best friend</td>\n",
       "      <td>did you have sex with your best friend</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352489</th>\n",
       "      <td>What are career option and job opportunities f...</td>\n",
       "      <td>How are job opportunities in Germany for an In...</td>\n",
       "      <td>what are career option and job opportun for me...</td>\n",
       "      <td>how are job opportun in germani for an indian ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227989</th>\n",
       "      <td>Which is the best novel that you have ever rea...</td>\n",
       "      <td>Novels: What are some of the best novels that ...</td>\n",
       "      <td>which is the best novel that you have ever rea...</td>\n",
       "      <td>novel : what are some of the best novel that y...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22684</th>\n",
       "      <td>From your perspective, what is the purpose of ...</td>\n",
       "      <td>What could be the basic purpose of life?</td>\n",
       "      <td>from your perspect what is the purpos of life</td>\n",
       "      <td>what could be the basic purpos of life</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111516</th>\n",
       "      <td>Is nitrogen good for tires?</td>\n",
       "      <td>What are some of the advantages of filling nit...</td>\n",
       "      <td>is nitrogen good for tire</td>\n",
       "      <td>what are some of the advantag of fill nitrogen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401324</th>\n",
       "      <td>Who is the most famous human being ever?</td>\n",
       "      <td>Who is most famous person who ever lived?</td>\n",
       "      <td>who is the most famous human be ever</td>\n",
       "      <td>who is most famous person who ever live</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       s1  \\\n",
       "166532   Why are people on this site so obsessed with IQ?   \n",
       "307615          Can your soul cause your body to explode?   \n",
       "171311  What are different fields in computer science ...   \n",
       "242855     How does it feel to have an IITian girlfriend?   \n",
       "187843       Have you ever had sex with your best friend?   \n",
       "...                                                   ...   \n",
       "352489  What are career option and job opportunities f...   \n",
       "227989  Which is the best novel that you have ever rea...   \n",
       "22684   From your perspective, what is the purpose of ...   \n",
       "111516                        Is nitrogen good for tires?   \n",
       "401324           Who is the most famous human being ever?   \n",
       "\n",
       "                                                       s2  \\\n",
       "166532  Why are most Quora users so obsessed with ques...   \n",
       "307615  Is it true that the soul can cause the body to...   \n",
       "171311    What are different fields for computer science?   \n",
       "242855  How does it feel to have an IITian as a girlfr...   \n",
       "187843            Did you have sex with your best friend?   \n",
       "...                                                   ...   \n",
       "352489  How are job opportunities in Germany for an In...   \n",
       "227989  Novels: What are some of the best novels that ...   \n",
       "22684            What could be the basic purpose of life?   \n",
       "111516  What are some of the advantages of filling nit...   \n",
       "401324          Who is most famous person who ever lived?   \n",
       "\n",
       "                                             s1_processed  \\\n",
       "166532       whi are peopl on this site so obsess with iq   \n",
       "307615             can your soul caus your bodi to explod   \n",
       "171311  what are differ field in comput scienc and whi...   \n",
       "242855       how doe it feel to have an iitian girlfriend   \n",
       "187843        have you ever had sex with your best friend   \n",
       "...                                                   ...   \n",
       "352489  what are career option and job opportun for me...   \n",
       "227989  which is the best novel that you have ever rea...   \n",
       "22684       from your perspect what is the purpos of life   \n",
       "111516                          is nitrogen good for tire   \n",
       "401324               who is the most famous human be ever   \n",
       "\n",
       "                                             s2_processed  y_true  y_naiv  \n",
       "166532  whi are most quora user so obsess with questio...       1       0  \n",
       "307615  is it true that the soul can caus the bodi to ...       1       0  \n",
       "171311            what are differ field for comput scienc       1       0  \n",
       "242855  how doe it feel to have an iitian as a girlfriend       0       1  \n",
       "187843             did you have sex with your best friend       1       0  \n",
       "...                                                   ...     ...     ...  \n",
       "352489  how are job opportun in germani for an indian ...       1       0  \n",
       "227989  novel : what are some of the best novel that y...       1       0  \n",
       "22684              what could be the basic purpos of life       1       0  \n",
       "111516  what are some of the advantag of fill nitrogen...       1       0  \n",
       "401324            who is most famous person who ever live       1       0  \n",
       "\n",
       "[27400 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.y_true - df.y_naiv).abs() > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c79e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ad\" in get_topic_probs.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e543c6b",
   "metadata": {},
   "source": [
    "### Naive cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_lg\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"tok2vec\"] = df[[\"s1\", \"s2\"]].apply(lambda row: nlp(row.s1).similarity(nlp(row.s2)), axis=1)\n",
    "# print(f\"SpearmanRank-val: {spearmanr(df.tok2vec, df.y_true)[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"Blue and red plane in mid-air flight.\")\n",
    "# print(\"\\t\".join([token.dep_ for token in doc]))\n",
    "# print(\"\\t\".join([token.lemma_ for token in doc]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2031cad5f309ecce3c7725635a5b7ce703908d09c65dbfde6df395d8b1d2cb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
